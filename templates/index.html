<!DOCTYPE html>
<html>

<head>
    <title>Vapi Voice Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }

        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }

        h1 {
            color: #333;
            text-align: center;
            margin-bottom: 20px;
        }

        .avatar-container {
            text-align: center;
            margin-bottom: 20px;
        }

        #avatar {
            width: 300px;
            height: 300px;
            border-radius: 50%;
            object-fit: cover;
        }

        .controls {
            text-align: center;
            margin-bottom: 30px;
        }

        button {
            background: #4CAF50;
            color: white;
            border: none;
            padding: 15px 30px;
            margin: 0 10px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
            transition: background 0.3s;
        }

        button:hover {
            background: #45a049;
        }

        button:disabled {
            background: #cccccc;
            cursor: not-allowed;
        }

        .stop-btn {
            background: #f44336;
        }

        .stop-btn:hover {
            background: #da190b;
        }

        .status {
            text-align: center;
            margin: 20px 0;
            padding: 10px;
            border-radius: 5px;
            font-weight: bold;
        }

        .status.connected {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }

        .status.disconnected {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }

        .transcript-container {
            border: 1px solid #ddd;
            border-radius: 10px;
            max-height: 500px;
            overflow-y: auto;
            background: #1a1a1a;
            padding: 20px;
            scroll-behavior: smooth;
        }

        .chat-message {
            margin-bottom: 20px;
            display: flex;
            align-items: flex-start;
            animation: fadeIn 0.3s ease-in;
        }

        .chat-message.user {
            justify-content: flex-end;
        }

        .chat-message.assistant {
            justify-content: flex-start;
        }

        .message-bubble {
            max-width: 70%;
            padding: 12px 16px;
            border-radius: 18px;
            position: relative;
            word-wrap: break-word;
        }

        .user .message-bubble {
            background: #007bff;
            color: white;
            border-bottom-right-radius: 5px;
        }

        .assistant .message-bubble {
            background: #2d2d2d;
            color: #00d4aa;
            border-bottom-left-radius: 5px;
            border: 1px solid #444;
        }

        .message-header {
            font-size: 12px;
            font-weight: bold;
            margin-bottom: 5px;
            opacity: 0.8;
        }

        .user .message-header {
            color: #b3d9ff;
        }

        .assistant .message-header {
            color: #00d4aa;
        }

        .message-text {
            line-height: 1.4;
            font-size: 14px;
        }

        .message-time {
            font-size: 11px;
            opacity: 0.6;
            margin-top: 5px;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .debug-toggle {
            margin: 20px 0;
            text-align: center;
        }

        .debug-logs {
            background: #1e1e1e;
            color: #00ff00;
            padding: 15px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            max-height: 200px;
            overflow-y: auto;
            white-space: pre-wrap;
            display: none;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Hi, I'm Maya!</h1>

        <div class="avatar-container">
            <img id="avatar" src="" alt="AI Avatar">
        </div>

        <div class="controls">
            <button id="startBtn" onclick="startCall()">Start Conversation</button>
            <button id="stopBtn" class="stop-btn" onclick="stopCall()" disabled>Stop Conversation</button>
        </div>

        <div id="status" class="status disconnected">Disconnected</div>

        <h3>Call Transcript</h3>
        <div class="transcript-container">
            <div id="transcript">
                <div class="chat-message assistant">
                    <div class="message-bubble">
                        <div class="message-header">Assistant</div>
                        <div class="message-text">Hi! I'm ready to help you. Click "Start Conversation" to begin.</div>
                        <div class="message-time">Ready to start</div>
                    </div>
                </div>
            </div>
        </div>

        <div class="debug-toggle">
            <button onclick="toggleDebug()">Toggle Debug Logs</button>
        </div>
        <div id="debugLogs" class="debug-logs"></div>
    </div>

    <script type="module">
        let socket;
        let audioContext;
        let micStream;
        let showDebug = false;
        let outputWorkletNode;
        let inputWorkletNode;
        let assistantSpeakingTimeout;
        let isAssistantSpeaking = false; // Track speaking state to avoid GIF reloading

        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const statusEl = document.getElementById('status');
        const transcriptEl = document.getElementById('transcript');
        const debugLogsEl = document.getElementById('debugLogs');

        const avatarImg = document.getElementById('avatar');
        const animatedAvatarSrc = "/static/img/avatar3.gif";
        const staticAvatarSrc = "/static/img/avatar_static_2.png";

        if (avatarImg) {
            avatarImg.src = staticAvatarSrc;
        }

        window.updateStatus = function (message, connected = false) {
            statusEl.textContent = message;
            statusEl.className = `status ${connected ? 'connected' : 'disconnected'}`;
            startBtn.disabled = connected;
            stopBtn.disabled = !connected;
        }

        window.addToTranscript = function (role, text, timestamp = null) {
            const chatMessage = document.createElement('div');
            chatMessage.className = `chat-message ${role}`;

            const messageBubble = document.createElement('div');
            messageBubble.className = 'message-bubble';

            const messageHeader = document.createElement('div');
            messageHeader.className = 'message-header';
            messageHeader.textContent = role === 'user' ? 'You' : 'Assistant';

            const messageText = document.createElement('div');
            messageText.className = 'message-text';
            messageText.textContent = text;

            const messageTime = document.createElement('div');
            messageTime.className = 'message-time';
            messageTime.textContent = timestamp || new Date().toLocaleTimeString();

            messageBubble.appendChild(messageHeader);
            messageBubble.appendChild(messageText);
            messageBubble.appendChild(messageTime);
            chatMessage.appendChild(messageBubble);

            transcriptEl.appendChild(chatMessage);

            setTimeout(() => {
                transcriptEl.scrollTo({ top: transcriptEl.scrollHeight, behavior: 'smooth' });
            }, 100);
        }

        window.debugLog = function (message, data = null) {
            const timestamp = new Date().toLocaleTimeString();
            let logMessage = `[${timestamp}] ${message}`;
            if (data) {
                logMessage += '\n' + JSON.stringify(data, null, 2);
            }
            if (showDebug) {
                debugLogsEl.textContent += logMessage + '\n';
                debugLogsEl.scrollTop = debugLogsEl.scrollHeight;
            }
            console.log(logMessage, data);
        }

        window.toggleDebug = function () {
            showDebug = !showDebug;
            debugLogsEl.style.display = showDebug ? 'block' : 'none';
        }

        function setAvatarSpeaking(speaking) {
            if (speaking && !isAssistantSpeaking) {
                // Start speaking - change to animated only if not already speaking
                isAssistantSpeaking = true;
                if (avatarImg) {
                    avatarImg.src = animatedAvatarSrc;
                }
                debugLog("Avatar started speaking animation");
            } else if (!speaking && isAssistantSpeaking) {
                // Stop speaking
                isAssistantSpeaking = false;
                if (avatarImg) {
                    avatarImg.src = staticAvatarSrc;
                }
                debugLog("Avatar stopped speaking animation");
            }
        }

        async function setupAudioWorklets() {
            if (!audioContext) {
                audioContext = new AudioContext({ sampleRate: 16000 });
                await audioContext.resume();
            }

            await audioContext.audioWorklet.addModule('/static/js/audio-output-worklet.js');
            outputWorkletNode = new AudioWorkletNode(audioContext, 'audio-output-processor');
            outputWorkletNode.connect(audioContext.destination);
            debugLog('Audio output worklet is ready.');

            await audioContext.audioWorklet.addModule('/static/js/audio-input-worklet.js');
            micStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    sampleRate: 16000,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true
                }
            });
            const micSourceNode = audioContext.createMediaStreamSource(micStream);
            inputWorkletNode = new AudioWorkletNode(audioContext, 'audio-input-processor');

            inputWorkletNode.port.onmessage = (event) => {
                if (socket && socket.readyState === WebSocket.OPEN) {
                    socket.send(event.data);
                }
            };

            micSourceNode.connect(inputWorkletNode);
            debugLog('Audio input worklet is ready and listening to microphone.');
        }

        window.startCall = async function () {
            try {
                updateStatus("Connecting...", false);
                debugLog("Initiating call request");

                const res = await fetch("/make_call");
                if (!res.ok) {
                    const error = await res.json();
                    throw new Error(error.detail || "Failed to create call");
                }
                const data = await res.json();

                if (!data.url) {
                    throw new Error("No WebSocket URL received");
                }

                debugLog("Received WebSocket URL", { url: data.url });

                socket = new WebSocket(data.url);
                socket.binaryType = "arraybuffer";

                socket.onopen = async () => {
                    debugLog("WebSocket connection opened");
                    updateStatus("Connected - Starting microphone...", true);
                    try {
                        await setupAudioWorklets();
                        updateStatus("Connected - Listening...", true);
                    } catch (micError) {
                        debugLog("Audio setup error", micError);
                        updateStatus(`Error: ${micError.message}`, false);
                        stopCall();
                    }
                };

                socket.onmessage = (event) => {
                    if (typeof event.data === "string") {
                        try {
                            const msg = JSON.parse(event.data);
                            handleJsonMessage(msg);
                        } catch (parseError) {
                            debugLog("JSON parse error", parseError);
                        }
                    } else if (event.data instanceof ArrayBuffer) {
                        if (outputWorkletNode) {
                            outputWorkletNode.port.postMessage(event.data);
                        }
                    }
                };

                socket.onerror = (error) => {
                    debugLog("WebSocket error", error);
                    updateStatus("Connection error", false);
                };

                socket.onclose = (event) => {
                    debugLog("WebSocket closed", { code: event.code, reason: event.reason });
                    updateStatus("Disconnected", false);
                    cleanup();
                };

            } catch (error) {
                debugLog("Start call error", error);
                updateStatus(`Failed to connect: ${error.message}`, false);
            }
        }

        function handleJsonMessage(msg) {
            debugLog("Received JSON message", msg);

            switch (msg.type) {
                case 'speech-update':
                    // Use speech-update events for smoother avatar control
                    if (msg.role === 'assistant') {
                        if (msg.status === 'started') {
                            setAvatarSpeaking(true);
                            // Clear any existing timeout
                            clearTimeout(assistantSpeakingTimeout);
                        } else if (msg.status === 'stopped') {
                            // Add a small delay before stopping animation to avoid flickering
                            clearTimeout(assistantSpeakingTimeout);
                            assistantSpeakingTimeout = setTimeout(() => {
                                setAvatarSpeaking(false);
                            }, 500);
                        }
                    }
                    break;

                case 'transcript':
                    // Only handle final transcripts for transcript display
                    if (msg.transcriptType === 'final' && msg.transcript) {
                        const role = msg.role === 'user' ? 'user' : 'assistant';
                        addToTranscript(role, msg.transcript);
                    }
                    break;

                case 'error':
                    debugLog("Error message received", msg);
                    updateStatus(`Error: ${msg.message || 'Unknown error'}`, false);
                    break;

                default:
                    break;
            }
        }

        window.stopCall = function () {
            debugLog("Stopping call");
            if (socket) {
                if (socket.readyState === WebSocket.OPEN) {
                    socket.send(JSON.stringify({ type: "hangup" }));
                }
                socket.close();
            }
            cleanup();
            updateStatus("Disconnected", false);

            // Reset avatar state
            setAvatarSpeaking(false);
            clearTimeout(assistantSpeakingTimeout);
        }

        function cleanup() {
            if (micStream) {
                micStream.getTracks().forEach(track => track.stop());
                micStream = null;
                debugLog("Microphone stream stopped.");
            }
            if (inputWorkletNode) {
                inputWorkletNode.disconnect();
                inputWorkletNode = null;
            }
            if (outputWorkletNode) {
                outputWorkletNode.disconnect();
                outputWorkletNode = null;
            }
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close().then(() => {
                    audioContext = null;
                    debugLog("AudioContext closed.");
                });
            }
            socket = null;
            isAssistantSpeaking = false;
            debugLog("Cleanup completed");
        }

        window.startCall = startCall;
        window.stopCall = stopCall;
        window.toggleDebug = toggleDebug;
    </script>
</body>

</html>